<!DOCTYPE html><html lang="default" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>CC-FPSE | 私人海域</title><meta name="description" content="Title：Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis Year：NeurIPS 2019 Author：Xihui Liu School：The Chinese University of HongKong Code：https:&#x2F;&#x2F;github.com&#x2F;xh-"><meta name="keywords" content="GAN"><meta name="author" content="Sonata,sonatau@163.com"><meta name="copyright" content="Sonata"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://tva1.sinaimg.cn/large/008i3skNly1gtytp9mlqyj60iz09y0sz02.jpg"><link rel="canonical" href="http://yluy.gitee.io/2021/08/30/CC-FPSE/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="CC-FPSE"><meta property="og:url" content="http://yluy.gitee.io/2021/08/30/CC-FPSE/"><meta property="og:site_name" content="私人海域"><meta property="og:description" content="Title：Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis Year：NeurIPS 2019 Author：Xihui Liu School：The Chinese University of HongKong Code：https:&#x2F;&#x2F;github.com&#x2F;xh-"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gtysla6wewj60rs0rsq8y02.jpg"><meta property="article:published_time" content="2021-08-30T06:17:09.000Z"><meta property="article:modified_time" content="2021-08-30T06:21:53.128Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.0.2',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2021-08-30 14:21:53'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 5.0.2"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghwjow3dndj30jg0jgdgq.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">77</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">15</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">23</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Homepage</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#topic-and-gap"><span class="toc-number">1.</span> <span class="toc-text"> Topic and Gap</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#topic"><span class="toc-number">1.1.</span> <span class="toc-text"> Topic</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gap"><span class="toc-number">1.2.</span> <span class="toc-text"> Gap</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#contributions"><span class="toc-number">2.</span> <span class="toc-text"> Contributions</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#method-details"><span class="toc-number">3.</span> <span class="toc-text"> Method Details</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#image-generation"><span class="toc-number">3.1.</span> <span class="toc-text"> Image Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#conditional-weight-predition"><span class="toc-number">3.2.</span> <span class="toc-text"> Conditional Weight Predition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#feature-pyramid-semantics-embedding-discriminator"><span class="toc-number">4.</span> <span class="toc-text"> Feature Pyramid Semantics Embedding Discriminator</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#semantic-embedding-for-discriminator"><span class="toc-number">4.1.</span> <span class="toc-text"> Semantic Embedding for Discriminator</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">5.</span> <span class="toc-text"> References</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#notes"><span class="toc-number">6.</span> <span class="toc-text"> Notes</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://tva1.sinaimg.cn/large/008i3skNly1gtysla6wewj60rs0rsq8y02.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">私人海域</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Homepage</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">CC-FPSE</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-08-30T06:17:09.000Z" title="Created 2021-08-30 14:17:09">2021-08-30</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-08-30T06:21:53.128Z" title="Updated 2021-08-30 14:21:53">2021-08-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/">阅读笔记</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/%E8%AE%BA%E6%96%87/">论文</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p><strong>Title</strong>：Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis<br />
<strong>Year</strong>：NeurIPS 2019<br />
<strong>Author</strong>：Xihui Liu<br />
<strong>School</strong>：The Chinese University of HongKong<br />
<strong>Code</strong>：<a target="_blank" rel="noopener" href="https://github.com/xh-liu/CC-FPSE">https://github.com/xh-liu/CC-FPSE</a></p>
<h2 id="topic-and-gap"><a class="markdownIt-Anchor" href="#topic-and-gap"></a> Topic and Gap</h2>
<h3 id="topic"><a class="markdownIt-Anchor" href="#topic"></a> Topic</h3>
<p>Semantic image synthesis, which aims at generating photorealistic images conditioned on semantic layouts.</p>
<h3 id="gap"><a class="markdownIt-Anchor" href="#gap"></a> Gap</h3>
<p><strong>How to exploit the semantic layout information in the generator</strong></p>
<ul>
<li>Preserve information: most of network just be fed into semantic label maps once at the input layers.</li>
<li>Spatial locations: distinct semantic labels at different locations as well as the unique semantic layout of each sample, different convolutional kernels should be used for generating different stuff or object.</li>
</ul>
<p><strong>How to promote high-fidelity details and enhance the spatial semantic alignment in the discriminator</strong></p>
<ul>
<li>High-fidelity details includes texture and edges</li>
<li>the spatial semantic alignment between generated image and semantic label layout</li>
<li>Current discriminator seems without considering whether generated image matches well with the label map.</li>
</ul>
<h2 id="contributions"><a class="markdownIt-Anchor" href="#contributions"></a> Contributions</h2>
<ul>
<li>Predict layout-to-image conditional convolution kernels based on the semantic layout for generating.</li>
<li>A feature pyramid semantics-embedding discriminator.</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gtym3qrstlj616q0j610q02.jpg" alt="" /></p>
<h2 id="method-details"><a class="markdownIt-Anchor" href="#method-details"></a> Method Details</h2>
<h3 id="image-generation"><a class="markdownIt-Anchor" href="#image-generation"></a> Image Generation</h3>
<p><strong>Model input</strong></p>
<ul>
<li>Noise Map with CC kernels from Weight Prediction Network</li>
</ul>
<p><strong>Kernel weight</strong><br />
For traditional convolution layers, they are applied to all samples and at all spatial locations, but different objects should be generated differently, the better way is, at each convolution layers and each multiplication times, these kernels weights are suitable for that locations, by given semantic label map.</p>
<p><strong>Depthwise separable convolution</strong><br />
In order to avoid excessive computational costs and GPU memory usage, the author uses “depthwise separable convolution” to predict weights for each layer.</p>
<ul>
<li><u>definition</u>: The depth wise separable convolutions consist of two steps: depthwise convolutions and 1x1 convolutions.</li>
<li><u>info details</u>: <a target="_blank" rel="noopener" href="https://towardsdatascience.com/a-comprehensive-introduction-to-different-types-of-convolutions-in-deep-learning-669281e58215">A Comprehensive Introduction to Different Types of Convolutions in Deep Learning</a></li>
<li><u>parameters</u>: for each layer, parameters number: <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>C</mi><mo>×</mo><mi>k</mi><mo>×</mo><mi>k</mi><mo>×</mo><mi>H</mi><mo>×</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">C \times k \times k \times H \times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.77777em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.76666em;vertical-align:-0.08333em;"></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">W</span></span></span></span></li>
</ul>
<p><strong>Conditional attention operation</strong></p>
<ul>
<li><u>role</u>: gate the information flow passed to the next layer.</li>
<li><u> computation </u>: the element-wise product, the same shape with the last output.</li>
</ul>
<h3 id="conditional-weight-predition"><a class="markdownIt-Anchor" href="#conditional-weight-predition"></a> Conditional Weight Predition</h3>
<p>The generator uses different kernels to generate images, but how to get the predicted weights?</p>
<p><strong>Model input</strong></p>
<ul>
<li>Label map</li>
</ul>
<p><strong>Predicted weights</strong><br />
Semantic label maps have some hidden information, such as locations between different stuff. A small receptive field restricts the weight prediction from incorporating long-range context information. So the author introduce “A feature pyramid structure”.</p>
<p><strong>A feature pyramid structure</strong></p>
<ul>
<li><u> process </u>: The features at different levels of the feature pyramid are concatenated with the original semantic map to obtain the global-context-aware semantic feature maps.</li>
<li><u>role</u>: predict weights which are aware of not only local neighborhood, but also long-range context and relative locations.</li>
</ul>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gtyogvi3gkj60r40dm0v902.jpg" alt="" /></p>
<h2 id="feature-pyramid-semantics-embedding-discriminator"><a class="markdownIt-Anchor" href="#feature-pyramid-semantics-embedding-discriminator"></a> Feature Pyramid Semantics Embedding Discriminator</h2>
<p><strong>Drawback of PathGAN</strong></p>
<ul>
<li>Just discriminate whether an image is fake or true, not to keep semantic alignment.</li>
</ul>
<p><strong>Model input</strong></p>
<ul>
<li>an image (GT or generated image)</li>
<li>the label map is used as an additional clues to compute a matching score.</li>
</ul>
<h3 id="semantic-embedding-for-discriminator"><a class="markdownIt-Anchor" href="#semantic-embedding-for-discriminator"></a> Semantic Embedding for Discriminator</h3>
<p><strong>PatchGAN</strong><br />
for each feature map, it classify if each patch is real or not by predicting a score for each spatial location.</p>
<p><strong>Projection discriminator</strong><br />
computes the dot product between the class label and image feature vector as part of the output discriminator score.</p>
<p><strong>Ours</strong></p>
<ul>
<li><u>role</u>: force the discriminator to classify not only real or fake images, but also <strong>whether the patch features match with the semantic labels in that patch</strong> within a joint embedding space.</li>
<li><u>info details</u>: 1)inspired by “Projection discriminator”, calculate the inner product between each spatial location of <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>F</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">F_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">F</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>S</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">S_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:-0.05764em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, to obtain a semantic matching score map; 2) The semantic matching score is <strong>added with</strong> the conventional real/fake score as the final discriminator score.</li>
</ul>
<h2 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h2>
<ol>
<li>Takeru Miyato and Masanori Koyama. cgans with projection discriminator. arXiv preprint arXiv:1802.05637, 2018.</li>
</ol>
<h2 id="notes"><a class="markdownIt-Anchor" href="#notes"></a> Notes</h2>
<p>I found that some papers you were hardly understand what the content means yesterday, would become easier to read tomorrow…</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:sonatau@163.com">Sonata</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yluy.gitee.io/2021/08/30/CC-FPSE/">http://yluy.gitee.io/2021/08/30/CC-FPSE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/GAN/">GAN</a></div><div class="post_share"><div class="social-share" data-image="https://tva1.sinaimg.cn/large/008i3skNly1gtysla6wewj60rs0rsq8y02.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/01/VAE/"><img class="prev-cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu13bvajnhj61540jgwiz02.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">VAE</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/26/SMIS/"><img class="next-cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gtu86uqlj7j60u00u0q8402.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">SMIS</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/08/26/Art2Real/" title="Art2Real"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gtu8328bcxj60u00u00w802.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-26</div><div class="relatedPosts_title">Art2Real</div></div></a></div><div class="relatedPosts_item"><a href="/2021/08/26/SMIS/" title="SMIS"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gtu86uqlj7j60u00u0q8402.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-08-26</div><div class="relatedPosts_title">SMIS</div></div></a></div><div class="relatedPosts_item"><a href="/2020/12/29/【论文阅读】SIGGRAPH2020《DeepFaceDrawing》草图到人脸图像/" title="【论文阅读】SIGGRAPH2020《DeepFaceDrawing》草图到人脸图像"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/0081Kckwly1gm4jiq9pdgj31900u0x6p.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-29</div><div class="relatedPosts_title">【论文阅读】SIGGRAPH2020《DeepFaceDrawing》草图到人脸图像</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://tva1.sinaimg.cn/large/008i3skNly1gtysla6wewj60rs0rsq8y02.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Sonata</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">hi there ~</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script defer="defer" id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script src="/js/third-party/click_heart.js" async="async"></script></div></body></html>