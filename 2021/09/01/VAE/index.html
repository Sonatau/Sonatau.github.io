<!DOCTYPE html><html lang="default" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>VAE | 私人海域</title><meta name="description" content="我这几天比较好奇VAE模型和简单AE之间的区别，就是为什么要引入&#x2F;mu&#x2F;mu&#x2F;mu和sigmasigmasigma，看了一上午，似懂非懂。  AutoEncoder  Encoder Produce the new features representation from the old features representation, from the initial space to late"><meta name="keywords" content="中等"><meta name="author" content="Sonata,sonatau@163.com"><meta name="copyright" content="Sonata"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://tva1.sinaimg.cn/large/008i3skNly1gwehay3b55j305k05k74a.jpg"><link rel="canonical" href="http://yluy.gitee.io/2021/09/01/VAE/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="VAE"><meta property="og:url" content="http://yluy.gitee.io/2021/09/01/VAE/"><meta property="og:site_name" content="私人海域"><meta property="og:description" content="我这几天比较好奇VAE模型和简单AE之间的区别，就是为什么要引入&#x2F;mu&#x2F;mu&#x2F;mu和sigmasigmasigma，看了一上午，似懂非懂。  AutoEncoder  Encoder Produce the new features representation from the old features representation, from the initial space to late"><meta property="og:image" content="https://tva1.sinaimg.cn/large/008i3skNly1gu13bvajnhj61540jgwiz02.jpg"><meta property="article:published_time" content="2021-09-01T05:59:34.000Z"><meta property="article:modified_time" content="2021-09-01T06:03:50.993Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  hexoversion: '5.0.2',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: undefined,
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: true,
  islazyload: true,
  isanchor: false    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2021-09-01 14:03:50'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><meta name="generator" content="Hexo 5.0.2"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gweh81b6pyj30u00u0q49.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">77</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">15</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">23</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Homepage</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#autoencoder"><span class="toc-number">1.</span> <span class="toc-text"> AutoEncoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#encoder"><span class="toc-number">1.1.</span> <span class="toc-text"> Encoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#decoder"><span class="toc-number">1.2.</span> <span class="toc-text"> Decoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#limitations"><span class="toc-number">1.3.</span> <span class="toc-text"> Limitations</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#variational-autoencoder"><span class="toc-number">2.</span> <span class="toc-text"> Variational autoencoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#definition"><span class="toc-number">2.1.</span> <span class="toc-text"> Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#loss-function"><span class="toc-number">2.2.</span> <span class="toc-text"> Loss function</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#about-regularisation"><span class="toc-number">2.3.</span> <span class="toc-text"> About regularisation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#references"><span class="toc-number">3.</span> <span class="toc-text"> References</span></a></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://tva1.sinaimg.cn/large/008i3skNly1gu13bvajnhj61540jgwiz02.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">私人海域</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Homepage</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">VAE</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2021-09-01T05:59:34.000Z" title="Created 2021-09-01 13:59:34">2021-09-01</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2021-09-01T06:03:50.993Z" title="Updated 2021-09-01 14:03:50">2021-09-01</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><p>我这几天比较好奇VAE模型和简单AE之间的区别，就是为什么要引入<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi mathvariant="normal">/</mi><mi>m</mi><mi>u</mi></mrow><annotation encoding="application/x-tex">/mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">/</span><span class="mord mathdefault">m</span><span class="mord mathdefault">u</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>s</mi><mi>i</mi><mi>g</mi><mi>m</mi><mi>a</mi></mrow><annotation encoding="application/x-tex">sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em;"></span><span class="mord mathdefault">s</span><span class="mord mathdefault">i</span><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="mord mathdefault">m</span><span class="mord mathdefault">a</span></span></span></span>，看了一上午，似懂非懂。</p>
<h2 id="autoencoder"><a class="markdownIt-Anchor" href="#autoencoder"></a> AutoEncoder</h2>
<h3 id="encoder"><a class="markdownIt-Anchor" href="#encoder"></a> Encoder</h3>
<p>Produce the new features representation from the old features representation, from the initial space to latent space, as dimensionality reduction.<br />
Aim: <strong>keep the maximum of information when encoding</strong></p>
<h3 id="decoder"><a class="markdownIt-Anchor" href="#decoder"></a> Decoder</h3>
<p>Decompress the latent vector back to the initial space, and recover more information as far as possioble.<br />
Aim: <strong>keep the minimum of reconstruction error when decoding</strong></p>
<p>At each iteration, the loss function could be illustrated as</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12smkasdj60z20j6jsh02.jpg" alt="" /></p>
<p>Indeed, if our encoder and decoder have enough degrees of freedom, we can reduce any initial dimensionality to <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mn>1</mn><mtext> </mtext><mi>N</mi></mrow><annotation encoding="application/x-tex">1~N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">1</span><span class="mspace nobreak"> </span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span> with a small loss.</p>
<p>We should however keep two things in mind:</p>
<ol>
<li>An important dimensionality reduction with no reconstrction loss often comes with a price: the lack of regularity in the latent space. ( dimension 1 )</li>
<li>Most of the time the final purpose of dimensionality reduction is not to only reduce the number of dimensions of the data but to <strong>reduce this number of dimensions while keeping the major part of the data structure information in the reduced representations</strong>.</li>
</ol>
<p>For these two reasons, the dimension of the latent space and the “depth” of autoencoders (that define degree and quality of compression) have to be carefully controlled and adjusted depending on the final purpose of the dimensionality reduction.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12svzy46j61ef0jydhc02.jpg" alt="" /></p>
<h3 id="limitations"><a class="markdownIt-Anchor" href="#limitations"></a> Limitations</h3>
<p>If the latent space is regular enough, we could take a point randomly from that latent space and decode it to get a new content we may need. The decoder would then act more or less like the generator of GAN.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12t4npt4j612l0k30tx02.jpg" alt="" /></p>
<p>The quality and relevance of generated content depend on the regularity of the latent space. But actually it is difficult to ensure that the encoder will organize the latent space to keep its regularity for autoencoder.</p>
<p>From the image as follow, it illustrates that a good model aims to find a correct mapping from the datasets distribution to the source distribution, in other words, it likes a brige between distributions.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu0y8zpjx8j60or0c775702.jpg" alt="" /></p>
<p>我卡住了，用中文吧：</p>
<p>这个模型能够将原来的概率分布映射到训练集的概率分布，也就是说，<strong>它们的目的都是进行分布之间的变换</strong>。生成模型的难题就是判断生成分布与真实分布的相似度。如果两者无法尽可能拟合，将导致生成分布sample出来的随机变量无法生成出合适的结果。</p>
<p>overfitting也是表现之一，即生成分布与真实分布相似度太低了。</p>
<p>The high degree of freedom of the autoencoder that makes possible to encode and decode with no infomation loss leads to a severe <strong>overfitting</strong>, implying that some points of the latent space will give meaningless content once decoded. Irregular latent space prevent us from using autoencoder for new content generation.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12ti5wznj61e40gojsv02.jpg" alt="" /></p>
<p><strong>the autoencoder is solely trained to encode and decode with as few loss as possible, no matter how the latent space is organised.</strong> Thus, if we are not careful about the definition of the architecture, it is natural that, during the training, the network takes advantage of any overfitting possibilities to achieve its task as well as it can…</p>
<h2 id="variational-autoencoder"><a class="markdownIt-Anchor" href="#variational-autoencoder"></a> Variational autoencoder</h2>
<p>To overcome the aforementioned drawbacks, we have to be sure that the latent space is regular enough.</p>
<h3 id="definition"><a class="markdownIt-Anchor" href="#definition"></a> Definition</h3>
<p><strong>A variational autoencoder can be defined as being an autoencoder whose training is regularised to avoid overfitting and ensure that the latent space has good properties that enable generative process</strong>.</p>
<p>In order to introduce some regularisation of the latent space, instead of encoding an input as a single point, <strong>we encode it as a distribution over the latent space</strong>!!!</p>
<p>The model is trained as follows:</p>
<ol>
<li>The input is encoded as distribution over the latent space</li>
<li>A point from the latent space is sampled from that distribution</li>
<li>The sampled point is decoded and the reconstruction error can be computed</li>
<li>The reconstruction error is backpropageted through the network</li>
</ol>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12tphkgmj61e40gk0tq02.jpg" alt="" /></p>
<p>The distribution returned by the encoder are enforced to be close to a standard normal distribution so as the latent space regularisation.</p>
<h3 id="loss-function"><a class="markdownIt-Anchor" href="#loss-function"></a> Loss function</h3>
<p>The loss function is composed of a “reconstrucion term” and a “regularisation term”, the later item which tends to regularise the organisation of the latent space by making the distributions returned by the encoder close to a standard normal distribution, is expressed by KLD loss.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12uaw5f6j612w0iw3zq02.jpg" alt="" /></p>
<h3 id="about-regularisation"><a class="markdownIt-Anchor" href="#about-regularisation"></a> About regularisation</h3>
<p>The regularity that is expected from the latent space in order to make generative process possible can be expressed through two main properties:</p>
<ol>
<li><strong>continuity</strong> (two close points in the latent space should not give two completely different contents once decoded)</li>
<li><strong>completeness</strong> (for a chosen distribution, a point sampled from the latent space should give “meaningful” content once decoded).</li>
</ol>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12uid1jpj61fz0izta702.jpg" alt="" /></p>
<p>The only fact that VAEs encode inputs as distributions instead of simple points is not sufficient to ensure continuity and completeness. Without a well defined regularisation term, the model can learn, in order to minimise its reconstruction error, <strong>to “ignore” the fact that distributions are returned and behave almost like classic autoencoders</strong> (leading to overfitting). To do so, the encoder can either return distributions with tiny variances (that would tend to be punctual distributions) or return distributions with very different means (that would then be really far apart from each other in the latent space). In both cases, distributions are used the wrong way (cancelling the expected benefit) and continuity and/or completeness are not satisfied.</p>
<p>So, in order to avoid these effects <strong>we have to regularise both the covariance matrix and the mean of the distributions returned by the encoder</strong>. In practice, this regularisation is done by enforcing distributions to be close to a standard normal distribution (centred and reduced). This way, we require the covariance matrices to be close to the identity, preventing punctual distributions, and the mean to be close to 0, preventing encoded distributions to be too far apart from each others.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12uu15pnj61fz0izq3u02.jpg" alt="" /></p>
<p>We can observe that continuity and completeness obtained with regularisation <strong>tend to create a “gradient” over the information encoded in the latent space</strong>. For example, a point of the latent space that would be halfway between the means of two encoded distributions coming from different training data should be decoded in something that is somewhere between the data that gave the first distribution and the data that gave the second distribution as it may be sampled by the autoencoder in both cases.</p>
<p><img src= "/img/loading.gif" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gu12zaueibj60z60izwf102.jpg" alt="" /></p>
<h2 id="references"><a class="markdownIt-Anchor" href="#references"></a> References</h2>
<ol>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/34998569">https://zhuanlan.zhihu.com/p/34998569</a></li>
<li><a target="_blank" rel="noopener" href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/27549418">https://zhuanlan.zhihu.com/p/27549418</a></li>
</ol>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:sonatau@163.com">Sonata</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://yluy.gitee.io/2021/09/01/VAE/">http://yluy.gitee.io/2021/09/01/VAE/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E4%B8%AD%E7%AD%89/">中等</a></div><div class="post_share"><div class="social-share" data-image="https://tva1.sinaimg.cn/large/008i3skNly1gu13bvajnhj61540jgwiz02.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2021/09/15/2021%E5%B9%B49%E6%9C%8815%E6%97%A5/"><img class="prev-cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1guhlizhvcgj61900u0n2902.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">2021年9月15日</div></div></a></div><div class="next-post pull-right"><a href="/2021/08/30/CC-FPSE/"><img class="next-cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gtysla6wewj60rs0rsq8y02.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">CC-FPSE</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2021/04/16/Attention机制在图像处理中的应用/" title="Attention机制在图像处理中的应用"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008eGmZEly1gpldjeoh5dj31400u0e83.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-16</div><div class="relatedPosts_title">Attention机制在图像处理中的应用</div></div></a></div><div class="relatedPosts_item"><a href="/2021/11/05/Boyer-Moore投票算法/" title="Boyer-Moore投票算法"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gw49zfypmwj31h70u04a1.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-11-05</div><div class="relatedPosts_title">Boyer-Moore投票算法</div></div></a></div><div class="relatedPosts_item"><a href="/2021/06/16/GauGAN网络结构与代码解析/" title="GauGAN网络结构与代码解析"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1grk46vwlabj30u00ssmyq.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-06-16</div><div class="relatedPosts_title">GauGAN网络结构与代码解析</div></div></a></div><div class="relatedPosts_item"><a href="/2021/05/05/Mac升级BigSur后提示没有权限打开应用程序/" title="Mac升级BigSur后提示没有权限打开应用程序"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gq7v3jth45j30jg0e4di0.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-05-05</div><div class="relatedPosts_title">Mac升级BigSur后提示没有权限打开应用程序</div></div></a></div><div class="relatedPosts_item"><a href="/2021/04/21/Sketch-Simplification线稿简化/" title="Sketch_Simplification线稿简化"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008i3skNly1gpsvjxm2vcj31900u01kz.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-21</div><div class="relatedPosts_title">Sketch_Simplification线稿简化</div></div></a></div><div class="relatedPosts_item"><a href="/2021/04/20/Trapped-Ball-Segmentation陷球分割/" title="Trapped-Ball-Segmentation陷球分割"><img class="relatedPosts_cover" data-lazy-src="https://tva1.sinaimg.cn/large/008eGmZEly1gpqikhejn7j31960u0qv6.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2021-04-20</div><div class="relatedPosts_title">Trapped-Ball-Segmentation陷球分割</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://tva1.sinaimg.cn/large/008i3skNly1gu13bvajnhj61540jgwiz02.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021 By Sonata</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">hi there ~</div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><div class="js-pjax"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.js"></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex-copytex@latest/dist/katex-copytex.min.css"><script>$(function () {
  $('span.katex-display').wrap('<div class="katex-wrap"></div>')
})</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><script defer="defer" id="ribbon_piao" mobile="false" src="/js/third-party/piao.js"></script><script src="/js/third-party/click_heart.js" async="async"></script></div></body></html>